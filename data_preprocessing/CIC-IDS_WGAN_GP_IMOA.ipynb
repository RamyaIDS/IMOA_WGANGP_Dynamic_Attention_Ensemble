{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac16e8-16e5-4a93-8e17-d912c37ce377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d0b7c8c-451d-4c1a-a403-ed66f09adb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# There are eight datasets in total. They can be found here: https://www.kaggle.com/datasets/cicdataset/cicids2017\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4cd726-d731-47aa-813e-2d0d186ce362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2830743, 79)\n"
     ]
    }
   ],
   "source": [
    "df1=pd.read_csv(\"Wednesday-workingHours.pcap_ISCX.csv\")\n",
    "df2=pd.read_csv(\"Tuesday-WorkingHours.pcap_ISCX.csv\")\n",
    "df3=pd.read_csv(\"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\")\n",
    "df4=pd.read_csv(\"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n",
    "df5=pd.read_csv(\"Monday-WorkingHours.pcap_ISCX.csv\")\n",
    "df6=pd.read_csv(\"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n",
    "df7=pd.read_csv(\"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\n",
    "df8=pd.read_csv(\"Friday-WorkingHours-Morning.pcap_ISCX.csv\")\n",
    "df = pd.concat([df1,df2, df3, df4, df5, df6, df7, df8])\n",
    "del df1\n",
    "del df2\n",
    "del df3\n",
    "del df4\n",
    "del df5\n",
    "del df6\n",
    "del df7\n",
    "del df8\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6170bca-97e5-44c0-a619-bf9d217e5b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2830743, 79)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52bda326-1d92-4a22-afd4-636ccb77b917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns that are completely homogenous are\n",
      " Bwd PSH Flags\n",
      " Bwd URG Flags\n",
      "Fwd Avg Bytes/Bulk\n",
      " Fwd Avg Packets/Bulk\n",
      " Fwd Avg Bulk Rate\n",
      " Bwd Avg Bytes/Bulk\n",
      " Bwd Avg Packets/Bulk\n",
      "Bwd Avg Bulk Rate\n",
      "(2830743, 71)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns that are completely homogenous\n",
    "\n",
    "df_nums = df.select_dtypes(include=[np.number])\n",
    "print(\"The columns that are completely homogenous are\")\n",
    "for col in df_nums.columns:\n",
    "    if (df_nums[col].max()-df_nums[col].min() == 0):\n",
    "        print(col,)\n",
    "df.drop([col for col in df_nums.columns if df_nums[col].max()-df_nums[col].min() == 0], axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c58b14c-8c54-4b48-abdb-260dd08cc86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2829385, 71)\n"
     ]
    }
   ],
   "source": [
    "# Drop NA rows\n",
    "\n",
    "df.dropna(axis=0, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948ef4ae-37e7-43a5-804b-cca114dc4e54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Destination Port : 0\n",
      " Flow Duration : 0\n",
      " Total Fwd Packets : 0\n",
      " Total Backward Packets : 0\n",
      "Total Length of Fwd Packets : 0\n",
      " Total Length of Bwd Packets : 0\n",
      " Fwd Packet Length Max : 0\n",
      " Fwd Packet Length Min : 0\n",
      " Fwd Packet Length Mean : 0\n",
      " Fwd Packet Length Std : 0\n",
      "Bwd Packet Length Max : 0\n",
      " Bwd Packet Length Min : 0\n",
      " Bwd Packet Length Mean : 0\n",
      " Bwd Packet Length Std : 0\n",
      "Flow Bytes/s : 0\n",
      " Flow Packets/s : 0\n",
      " Flow IAT Mean : 0\n",
      " Flow IAT Std : 0\n",
      " Flow IAT Max : 0\n",
      " Flow IAT Min : 0\n",
      "Fwd IAT Total : 0\n",
      " Fwd IAT Mean : 0\n",
      " Fwd IAT Std : 0\n",
      " Fwd IAT Max : 0\n",
      " Fwd IAT Min : 0\n",
      "Bwd IAT Total : 0\n",
      " Bwd IAT Mean : 0\n",
      " Bwd IAT Std : 0\n",
      " Bwd IAT Max : 0\n",
      " Bwd IAT Min : 0\n",
      "Fwd PSH Flags : 0\n",
      " Fwd URG Flags : 0\n",
      " Fwd Header Length : 0\n",
      " Bwd Header Length : 0\n",
      "Fwd Packets/s : 0\n",
      " Bwd Packets/s : 0\n",
      " Min Packet Length : 0\n",
      " Max Packet Length : 0\n",
      " Packet Length Mean : 0\n",
      " Packet Length Std : 0\n",
      " Packet Length Variance : 0\n",
      "FIN Flag Count : 0\n",
      " SYN Flag Count : 0\n",
      " RST Flag Count : 0\n",
      " PSH Flag Count : 0\n",
      " ACK Flag Count : 0\n",
      " URG Flag Count : 0\n",
      " CWE Flag Count : 0\n",
      " ECE Flag Count : 0\n",
      " Down/Up Ratio : 0\n",
      " Average Packet Size : 0\n",
      " Avg Fwd Segment Size : 0\n",
      " Avg Bwd Segment Size : 0\n",
      " Fwd Header Length.1 : 0\n",
      "Subflow Fwd Packets : 0\n",
      " Subflow Fwd Bytes : 0\n",
      " Subflow Bwd Packets : 0\n",
      " Subflow Bwd Bytes : 0\n",
      "Init_Win_bytes_forward : 0\n",
      " Init_Win_bytes_backward : 0\n",
      " act_data_pkt_fwd : 0\n",
      " min_seg_size_forward : 0\n",
      "Active Mean : 0\n",
      " Active Std : 0\n",
      " Active Max : 0\n",
      " Active Min : 0\n",
      "Idle Mean : 0\n",
      " Idle Std : 0\n",
      " Idle Max : 0\n",
      " Idle Min : 0\n",
      " Label : 0\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "df_nums = df.select_dtypes(include=[np.number])\n",
    "df_nums = (df_nums-df_nums.min())/(df_nums.max()-df_nums.min())\n",
    "\n",
    "for col in df_nums.columns:\n",
    "    df[col] = df_nums[col]\n",
    "\n",
    "df.dropna(axis=0, inplace=True)\n",
    "for c in df.columns:\n",
    "    print(c + \" :\", df[c].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6083123-4170-48dd-9d55-422624e59f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2827876, 71)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "513f9646-bade-4e24-bb60-b8d3535a00ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct label names and their counts:\n",
      "BENIGN                        2271320\n",
      "DoS Hulk                       230124\n",
      "PortScan                       158804\n",
      "DDoS                           128025\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7935\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5796\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1956\n",
      "Web Attack � Brute Force         1507\n",
      "Web Attack � XSS                  652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "Name:  Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = df[' Label'].value_counts()\n",
    "print(\"Distinct label names and their counts:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79316fce-e059-4f70-95a0-b8a26d1a403c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
      "0           0.001221        0.000319            0.000000   \n",
      "1           0.005936        0.000004            0.000046   \n",
      "2           0.001343        0.000009            0.000041   \n",
      "3           0.005936        0.000127            0.000073   \n",
      "4           0.001343        0.000009            0.000036   \n",
      "\n",
      "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
      "0                 0.000003                 4.651163e-07   \n",
      "1                 0.000017                 1.333333e-05   \n",
      "2                 0.000021                 2.441860e-04   \n",
      "3                 0.000041                 2.675969e-04   \n",
      "4                 0.000021                 2.441860e-04   \n",
      "\n",
      "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
      "0                  9.153974e-09                0.000242   \n",
      "1                  4.973659e-07                0.003183   \n",
      "2                  4.805836e-06                0.063457   \n",
      "3                  1.016091e-05                0.052901   \n",
      "4                  4.808888e-06                0.063457   \n",
      "\n",
      "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
      "0                0.002581                 0.001010                0.000000   \n",
      "1                0.000000                 0.002632                0.004414   \n",
      "2                0.000000                 0.053023                0.088773   \n",
      "3                0.000000                 0.034180                0.059753   \n",
      "4                0.000000                 0.058914                0.097467   \n",
      "\n",
      "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
      "0  ...                    1.0          0.0          0.0          0.0   \n",
      "1  ...                    1.0          0.0          0.0          0.0   \n",
      "2  ...                    1.0          0.0          0.0          0.0   \n",
      "3  ...                    1.0          0.0          0.0          0.0   \n",
      "4  ...                    1.0          0.0          0.0          0.0   \n",
      "\n",
      "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
      "0          0.0        0.0        0.0        0.0        0.0       0  \n",
      "1          0.0        0.0        0.0        0.0        0.0       0  \n",
      "2          0.0        0.0        0.0        0.0        0.0       0  \n",
      "3          0.0        0.0        0.0        0.0        0.0       0  \n",
      "4          0.0        0.0        0.0        0.0        0.0       0  \n",
      "\n",
      "[5 rows x 71 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to categorical columns\n",
    "for col in [' Label']:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "# Display the first few rows of the encoded DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2425348-5b7f-42d8-a2d7-7df424a52237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct label names and their counts:\n",
      "0     2271320\n",
      "4      230124\n",
      "10     158804\n",
      "2      128025\n",
      "3       10293\n",
      "7        7935\n",
      "11       5897\n",
      "6        5796\n",
      "5        5499\n",
      "1        1956\n",
      "12       1507\n",
      "14        652\n",
      "9          36\n",
      "13         21\n",
      "8          11\n",
      "Name:  Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = df[' Label'].value_counts()\n",
    "print(\"Distinct label names and their counts:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67681d3f-2307-4976-bc55-78b83a7a406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting rows with specified labels\n",
    "selected_labels = [9,13,8]\n",
    "\n",
    "# Check the precision of the labels in the DataFrame\n",
    "\n",
    "selected_rows = df[df[' Label'].isin(selected_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba52203c-5534-4cb6-bc3d-9197d1d72c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 71)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b6f0962-aaac-4b6d-bb79-38f1c6f46eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = selected_rows.iloc[:, :-1].values  # Features (excluding the label column)\n",
    "y = selected_rows.iloc[:, -1].values   # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70912656-9b3d-4172-b6b0-93953385d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec0b612b-f1bb-4755-97d7-59278cfe2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "\n",
    "# Define WGAN-GP components\n",
    "def make_generator_model(input_dim, output_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, use_bias=False, input_shape=(input_dim,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(256, use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(output_dim, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model(input_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, input_shape=(input_dim,)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.reduce_mean(real_output)\n",
    "    fake_loss = tf.reduce_mean(fake_output)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "def gradient_penalty(discriminator, real_data, fake_data):\n",
    "    batch_size = real_data.shape[0]\n",
    "    alpha = tf.random.uniform([batch_size, 1], 0.0, 1.0)\n",
    "    real_data = tf.cast(real_data, tf.float32)\n",
    "    fake_data = tf.cast(fake_data, tf.float32)\n",
    "    diff = fake_data - real_data\n",
    "    interpolated = real_data + alpha * diff\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        pred = discriminator(interpolated)\n",
    "    grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return gp\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_data, generator, discriminator, generator_optimizer, discriminator_optimizer, batch_size, z_dim, gp_weight):\n",
    "    noise = tf.random.normal([batch_size, z_dim])\n",
    "    real_data = tf.cast(real_data, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        fake_data = generator(noise, training=True)\n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(fake_data, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        gp = gradient_penalty(discriminator, real_data, fake_data)\n",
    "        total_disc_loss = disc_loss + gp * gp_weight\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(total_disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "# WGAN-GP training function\n",
    "def train_wgan_gp(X, epochs, batch_size, z_dim, gp_weight, generator, discriminator, generator_optimizer, discriminator_optimizer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tf.cast(X, tf.float32)).shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for real_data in dataset:\n",
    "            if real_data.shape[0] != batch_size:\n",
    "                continue  # Skip the batch if it does not match the required batch size\n",
    "            train_step(real_data, generator, discriminator, generator_optimizer, discriminator_optimizer, batch_size, z_dim, gp_weight)\n",
    "\n",
    "    noise = tf.random.normal([X.shape[0], z_dim])\n",
    "    synthetic_data = generator(noise, training=False)\n",
    "    return synthetic_data\n",
    "\n",
    "# Set WGAN-GP parameters\n",
    "z_dim = 10\n",
    "gp_weight = 10.0\n",
    "batch_size = 1\n",
    "epochs = 10  # Adjust as needed\n",
    "\n",
    "# Create WGAN-GP models and optimizers\n",
    "generator = make_generator_model(z_dim, X.shape[1])\n",
    "discriminator = make_discriminator_model(X.shape[1])\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# Train WGAN-GP and generate synthetic data\n",
    "synthetic_X = train_wgan_gp(X, epochs, batch_size, z_dim, gp_weight, generator, discriminator, generator_optimizer, discriminator_optimizer)\n",
    "synthetic_y = np.random.choice(y, size=synthetic_X.shape[0])  # Assuming same distribution for labels\n",
    "\n",
    "# Combine real and synthetic data\n",
    "X_combined = np.vstack((X, synthetic_X))\n",
    "y_combined = np.hstack((y, synthetic_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d042f7fb-6a6d-450d-ae1d-c3d6101aacc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8704, 70)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ee9ec5e-3e56-4480-8268-8b46584cdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into a single DataFrame\n",
    "df_combined = pd.DataFrame(X_combined)\n",
    "df_combined['Label'] = y_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1917eb03-0e3e-48e3-a5b2-3b11645ca4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct label names and their counts:\n",
      "9     4327\n",
      "13    2586\n",
      "8     1791\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = df_combined['Label'].value_counts()\n",
    "print(\"Distinct label names and their counts:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "723d8d36-81c8-4d97-b09d-c733b0aee0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.columns=df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db8196f9-98df-47d1-b3f8-78f22096d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for the rows to drop\n",
    "mask = df[' Label'].isin([9, 13, 8])\n",
    "# Drop the rows\n",
    "df_filtered = df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89ee1fbe-a6a5-4b21-bc30-ee002a7a13b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2827808, 71)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b20d253-b056-44dc-9f4a-f213779ed606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2827876, 71)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa61e9cb-78c7-4b2b-8b63-1dbfa65a658c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2836512, 71)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new=pd.concat([df_filtered,df_combined],axis=0)\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d68939ad-2ba9-45b2-9def-839283686731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to CIC_IDS_2017_merged_filtered_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Specify the filename\n",
    "filename = 'CIC_IDS_2017_merged_filtered_data.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_new.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf005314-0c9e-432c-a20f-40da13f3a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values  # Features (excluding the label column)\n",
    "y = df.iloc[:, -1].values   # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "50b8bd60-18ef-470c-b682-8a22e67ef316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6052b2b2-bf8e-4d9d-8105-e70524d24811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  5  6  7  9 11 13 17 18 19 23 24 26 28 29 31 32 33 35 37 38 40 42\n",
      " 43 44 45 46 47 49 50 51 52 54 56 57 62 64 65 67]\n"
     ]
    }
   ],
   "source": [
    "# Indian Millipede Optimization Algorithm(IMOA) based feature selection\n",
    "# Define the fitness function for feature selection based on mutual information\n",
    "def fitness_function(mask):\n",
    "    selected_features = np.where(mask == 1)[0]\n",
    "    if len(selected_features) == 0:\n",
    "        return 0  # No features selected, so fitness is zero\n",
    "    \n",
    "    X_selected = X[:, selected_features]\n",
    "    mi = mutual_info_classif(X_selected, y, discrete_features='auto')\n",
    "    return np.sum(mi)  # Maximize the sum of mutual information of selected features\n",
    "\n",
    "# Define the constraint violation function (if any)\n",
    "def constraint_violation(mask):\n",
    "    return 0  # No constraints in this case\n",
    "\n",
    "# Initialize parameters\n",
    "N = 20  # Population size\n",
    "d = X.shape[1]  # Dimensionality of the problem (number of features)\n",
    "T = 100  # Maximum iterations\n",
    "T_th = 0.5  # Temperature threshold\n",
    "α = 0.1  # Seasonal activity factor\n",
    "β = 0.5  # Reversal factor\n",
    "γ = 0.1  # Learning rate\n",
    "δ = 0.01  # Step size\n",
    "ε = 0.1  # Social factor\n",
    "λ = 10  # Penalty coefficient\n",
    "η = 0.5  # Crossover coefficient\n",
    "convergence_threshold = 1e-6  # Convergence threshold for fitness improvement\n",
    "no_improvement_limit = 10  # Number of iterations to wait for improvement before stopping\n",
    "\n",
    "# Initialize population\n",
    "P = np.random.randint(0, 2, (N, d))  # Binary mask for feature selection\n",
    "\n",
    "# Evaluate initial fitness\n",
    "fitness = np.array([fitness_function(mask) for mask in P])\n",
    "penalized_fitness = fitness - λ * np.array([constraint_violation(mask) for mask in P])\n",
    "\n",
    "# Main IMOA loop\n",
    "t = 0\n",
    "no_improvement_count = 0\n",
    "best_fitness = np.max(penalized_fitness)\n",
    "best_solution = P[np.argmax(penalized_fitness)]\n",
    "\n",
    "while t < T and no_improvement_count < no_improvement_limit:\n",
    "    for i in range(N):\n",
    "        # Seasonal Abundance\n",
    "        α_t = α * np.sin(2 * np.pi * t / T)\n",
    "        P[i] = np.clip(P[i] + α_t * np.random.uniform(-1, 1, d), 0, 1).astype(int)\n",
    "        \n",
    "        # Obstacle Avoidance\n",
    "        if penalized_fitness[i] < np.mean(penalized_fitness):  # Assuming poor fitness as below mean fitness\n",
    "            P[i] = np.clip(P[i] - β * np.random.uniform(-1, 1, d), 0, 1).astype(int)\n",
    "        \n",
    "        # Temperature Response\n",
    "        if np.random.uniform(0, 1) > T_th:\n",
    "            best_idx = np.argmax(penalized_fitness)\n",
    "            P[i] = np.clip(P[i] + γ * (P[best_idx] - P[i]), 0, 1).astype(int)\n",
    "        \n",
    "        # Resource Utilization\n",
    "        gradient = (fitness_function(P[i] + δ) - fitness_function(P[i])) / δ\n",
    "        P[i] = np.clip(P[i] + δ * gradient, 0, 1).astype(int)\n",
    "        \n",
    "        # Group Movement\n",
    "        P[i] = np.clip(P[i] + ε * (np.mean(P, axis=0) - P[i]), 0, 1).astype(int)\n",
    "        \n",
    "        # Defensive Behavior\n",
    "        if penalized_fitness[i] < np.mean(penalized_fitness):  # Assuming poor fitness as below mean fitness\n",
    "            penalized_fitness[i] -= λ * constraint_violation(P[i])\n",
    "        \n",
    "        # Mating Behavior\n",
    "        mate_idx = np.random.randint(N)\n",
    "        P[i] = np.clip(η * P[i] + (1 - η) * P[mate_idx], 0, 1).astype(int)\n",
    "        \n",
    "        # Predator Avoidance\n",
    "        if np.std(P) < 1e-6:  # Assuming low diversity as low standard deviation\n",
    "            P[i] = np.random.randint(0, 2, d)\n",
    "    \n",
    "    # Evaluate fitness of new positions\n",
    "    fitness = np.array([fitness_function(mask) for mask in P])\n",
    "    penalized_fitness = fitness - λ * np.array([constraint_violation(mask) for mask in P])\n",
    "    \n",
    "    # Check for convergence\n",
    "    current_best_fitness = np.max(penalized_fitness)\n",
    "    if current_best_fitness > best_fitness + convergence_threshold:\n",
    "        best_fitness = current_best_fitness\n",
    "        best_solution = P[np.argmax(penalized_fitness)]\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    # Increment iteration counter\n",
    "    t += 1\n",
    "\n",
    "selected_features = np.where(best_solution == 1 )[0]\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "80fd1e2e-3de6-4084-b36d-81ff2f92594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of selected features 40\n"
     ]
    }
   ],
   "source": [
    "num_entries = len( selected_features)\n",
    "print(\"Total number of selected features\",num_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0bcdb5f8-0b70-40a4-a7c1-6c92f4640acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Backward Packets\n",
      "Total Length of Fwd Packets\n",
      " Total Length of Bwd Packets\n",
      " Fwd Packet Length Max\n",
      " Fwd Packet Length Min\n",
      " Fwd Packet Length Std\n",
      " Bwd Packet Length Min\n",
      " Bwd Packet Length Std\n",
      " Flow IAT Std\n",
      " Flow IAT Max\n",
      " Flow IAT Min\n",
      " Fwd IAT Max\n",
      " Fwd IAT Min\n",
      " Bwd IAT Mean\n",
      " Bwd IAT Max\n",
      " Bwd IAT Min\n",
      " Fwd URG Flags\n",
      " Fwd Header Length\n",
      " Bwd Header Length\n",
      " Bwd Packets/s\n",
      " Max Packet Length\n",
      " Packet Length Mean\n",
      " Packet Length Variance\n",
      " SYN Flag Count\n",
      " RST Flag Count\n",
      " PSH Flag Count\n",
      " ACK Flag Count\n",
      " URG Flag Count\n",
      " CWE Flag Count\n",
      " Down/Up Ratio\n",
      " Average Packet Size\n",
      " Avg Fwd Segment Size\n",
      " Avg Bwd Segment Size\n",
      "Subflow Fwd Packets\n",
      " Subflow Bwd Packets\n",
      " Subflow Bwd Bytes\n",
      "Active Mean\n",
      " Active Max\n",
      " Active Min\n",
      " Idle Std\n"
     ]
    }
   ],
   "source": [
    "for feature_index in selected_features:\n",
    "    print(df.columns[feature_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
